# -*- coding: utf-8 -*-
"""Agentic Rag.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q8_A3uzIticXdjB90mTwLFxwcSgVo_Rg
"""

!pip install -qU smolagents langchain langchain-community langchain-huggingface sentence-transformers faiss-cpu pypdf rank_bm25 transformers accelerate langchain-text-splitters

import json, time, re, os, math, numpy as np
from google.colab import files
from typing import List, Dict

from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever
from transformers import pipeline

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

from smolagents import Tool, CodeAgent, InferenceClientModel

uploaded = files.upload()
pdf_path = list(uploaded.keys())[0]

docs = PyPDFLoader(pdf_path).load()
splitter = RecursiveCharacterTextSplitter(
    chunk_size=700, chunk_overlap=120, add_start_index=True, strip_whitespace=True
)
chunks = splitter.split_documents(docs)
for ch in chunks:
    ch.metadata["page"] = ch.metadata.get("page", ch.metadata.get("source", ""))

print(f"Prepared {len(chunks)} chunks")

emb = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")
vs = FAISS.from_documents(chunks, embedding=emb)
bm25 = BM25Retriever.from_documents(chunks)
bm25.k = 10

rerank_model_id = "cross-encoder/ms-marco-MiniLM-L-6-v2"
tok = AutoTokenizer.from_pretrained(rerank_model_id)
rerank_model = AutoModelForSequenceClassification.from_pretrained(rerank_model_id)
rerank_model.eval()
if torch.cuda.is_available():
    rerank_model.cuda()

def ce_score(q, p):
    """Compute similarity score using cross-encoder."""
    inputs = tok([q], [p], padding=True, truncation=True, return_tensors="pt")
    if torch.cuda.is_available():
        inputs = {k: v.cuda() for k, v in inputs.items()}
    with torch.no_grad():
        s = rerank_model(**inputs).logits.squeeze(-1)
    return float(s.item())

def route_intent(q: str):
    ql = q.lower()
    if any(w in ql for w in ["define", "what is", "meaning", "concept"]):
        return "definition"
    if any(w in ql for w in ["compare", "versus", "difference"]):
        return "comparison"
    if any(w in ql for w in ["how to", "steps", "pipeline", "implement"]):
        return "howto"
    return "generic"

def decompose(q: str) -> List[str]:
    parts = re.split(r"\?|\band\b|;|,", q)
    return [p.strip() for p in parts if p.strip()]

def rewrite(q: str):
    q2 = q.strip().rstrip("?")
    expansions = {
        "mesa": ["Modules for Experiments in Stellar Astrophysics"],
        "stellar": ["star", "stars", "stellar structure", "stellar evolution"],
        "module": ["component", "library", "subroutine", "package"],
        "evolution": ["evolutionary track", "time evolution", "evolutionary model"],
        "diffusion": ["mixing", "chemical diffusion", "composition transport"],
        "rotation": ["angular momentum", "spin", "rotational velocity", "rotational mixing"],
        "convection": ["convective zone", "energy transport"],
        "accretion": ["mass transfer", "accreting", "infall", "mass gain"],
        "white dwarf": ["WD", "degenerate star", "compact remnant"],
        "numerical": ["computational", "solver", "algorithmic"],
        "test suite": ["validation suite", "verification test", "code comparison"],
        "physics": ["microphysics", "macrophysics", "equation of state", "opacity"],
    }

    extra = []
    for k, vals in expansions.items():
        if re.search(rf"\b{k}\b", q2, flags=re.I):
            extra += vals
    if extra:
        q2 = q2 + " " + " ".join(set(extra))
    return q2

MEMO_PATH = "agent_memory.json"
if not os.path.exists(MEMO_PATH):
    with open(MEMO_PATH, "w") as f:
        json.dump({"episodes": []}, f)

def remember(query: str, answer: str, sources: List[Dict]):
    with open(MEMO_PATH, "r") as f:
        mem = json.load(f)
    mem["episodes"].append({
        "ts": time.time(),
        "query": query,
        "answer": answer,
        "sources": [
            {"page": s.metadata.get("page"),
             "start": s.metadata.get("start_index"),
             "preview": s.page_content[:160]} for s in sources]
    })
    with open(MEMO_PATH, "w") as f:
        json.dump(mem, f)

def episodic_recall(query: str, top_n: int = 2):
    with open(MEMO_PATH, "r") as f:
        mem = json.load(f)
    q_tokens = set(re.findall(r"\w+", query.lower()))
    scored = []
    for ep in mem.get("episodes", [])[-30:]:
        ep_tokens = set(re.findall(r"\w+", ep["query"].lower()))
        jacc = len(q_tokens & ep_tokens) / max(1, len(q_tokens | ep_tokens))
        scored.append((jacc, ep))
    scored.sort(key=lambda x: x[0], reverse=True)
    return [ep for _, ep in scored[:top_n] if _ > 0]

class VectorRetrieverTool(Tool):
    name = "dense_retriever"
    description = "Semantic vector retrieval over document chunks."
    inputs = {"query": {"type": "string", "description": "User query text"}}
    output_type = "string"

    def forward(self, query: str):
        docs = vs.similarity_search(query, k=8)
        return json.dumps([{"page": d.metadata.get("page"), "text": d.page_content[:500]} for d in docs])

class BM25RetrieverTool(Tool):
    name = "sparse_retriever"
    description = "Keyword-based BM25 retriever."
    inputs = {"query": {"type": "string", "description": "User query text"}}
    output_type = "string"

    def forward(self, query: str):
        docs = bm25.invoke(query)
        return json.dumps([{"page": d.metadata.get("page"), "text": d.page_content[:500]} for d in docs])

class RerankTool(Tool):
    name = "cross_encoder_rerank"
    description = "Reranks documents using a cross-encoder relevance model."
    inputs = {
        "query": {"type": "string", "description": "User question text"},
        "docs": {"type": "string", "description": "JSON list of retrieved docs"},
    }
    output_type = "string"

    def forward(self, query: str, docs: str):
        docs = json.loads(docs)
        scored = [(d, ce_score(query, d["text"])) for d in docs]
        scored.sort(key=lambda x: x[1], reverse=True)
        return json.dumps([d for d, _ in scored[:8]])

class ComposeContextTool(Tool):
    name = "compose_context"
    description = "Orders and summarizes docs into a compact context window."
    inputs = {
        "docs": {
            "type": "string",
            "description": "JSON list of reranked docs"
        },
        "budget": {
            "type": "number",
            "description": "Maximum token budget (optional)",
            "nullable": True
        },
    }
    output_type = "string"

    def forward(self, docs: str, budget: int = 1600):
        docs = json.loads(docs)
        buff, total = [], 0
        for d in docs:
            text = d["text"]
            if total + len(text) > budget:
                break
            buff.append(text)
            total += len(text)
        return "\n\n".join(buff)

class QueryPlanTool(Tool):
    name = "query_planner"
    description = "Rewrites, classifies, and decomposes the input query."
    inputs = {"query": {"type": "string", "description": "User input query"}}
    output_type = "string"

    def forward(self, query: str):
        q_rew = rewrite(query)
        intent = route_intent(query)
        parts = decompose(query)
        return json.dumps({"rewrite": q_rew, "intent": intent, "parts": parts})

class SelfReflectTool(Tool):
    name = "self_reflect"
    description = "Performs a simple overlap check between query and context."
    inputs = {
        "query": {"type": "string", "description": "Original user query"},
        "context": {"type": "string", "description": "Generated context window"},
        "docs": {"type": "string", "description": "JSON list of supporting docs"},
    }
    output_type = "string"

    def forward(self, query: str, context: str, docs: str):
        q_tokens = set(re.findall(r"[a-zA-Z]{3,}", query.lower()))
        ctx_tokens = set(re.findall(r"[a-zA-Z]{3,}", context.lower()))
        overlap = len(q_tokens & ctx_tokens) / max(1, len(q_tokens))
        decision = "good" if overlap >= 0.5 else "retry"
        return json.dumps({"overlap": round(overlap, 3), "decision": decision})

tools = [
    QueryPlanTool(),
    VectorRetrieverTool(),
    BM25RetrieverTool(),
    RerankTool(),
    ComposeContextTool(),
    SelfReflectTool(),
]

agent = CodeAgent(
    tools=tools,
    model=InferenceClientModel(),
    max_steps=6,
    verbosity_level=2
)

def agentic_rag(query: str):
    plan = json.loads(tools[0].forward(query=query))
    q_rew, intent = plan["rewrite"], plan["intent"]
    dense = json.loads(tools[1].forward(query=q_rew))
    sparse = json.loads(tools[2].forward(query=q_rew))
    merged = dense + sparse
    reranked = json.loads(tools[3].forward(query=q_rew, docs=json.dumps(merged)))
    context = tools[4].forward(docs=json.dumps(reranked), budget=1700)
    reflex = json.loads(tools[5].forward(query=query, context=context, docs=json.dumps(reranked)))

    if reflex["decision"] == "retry":
        extra = vs.similarity_search(q_rew, k=12)
        merged += [{"page": d.metadata.get("page"), "text": d.page_content[:400]} for d in extra]
        reranked = json.loads(tools[3].forward(query=q_rew, docs=json.dumps(merged)))
        context = tools[4].forward(docs=json.dumps(reranked), budget=1700)

    answer = f"Intent: {intent}\n\nContext (composed):\n{context}"
    remember(query, answer, [])
    return answer, reranked

query = "Summarize how MESA’s modular design allows integration of new physical modules (e.g., rotation or diffusion) and propose how an agent could extend it to model accreting white dwarfs."
answer, srcs = agentic_rag(query)

print("\n=== AGENTIC RAG — Final ===\n")
print(answer[:1800])
print("\nPages cited:", [s.get("page") for s in srcs[:6]])
print("\nEpisodic memory peek:", episodic_recall(query, top_n=1))

final_context = answer.split("Context (composed):", 1)[-1].strip()

qa_prompt = f"""
You are an expert astrophysicist and AI research assistant.
Using the following context extracted by the Agentic RAG pipeline,
compose a clear, concise, and factual answer to the user's question.

Context:
{final_context}

Question:
{query}

Answer:
"""

llm = pipeline(
    "text2text-generation",
    model="google/flan-t5-xl",
    device=0 if torch.cuda.is_available() else -1
)
final_answer = llm(qa_prompt, max_length=800, do_sample=False, temperature=0.3)[0]["generated_text"]

print("\n=== FINAL ANSWER (AGENTIC RAG) ===\n")
print(final_answer)
remember(query, final_answer, [])

